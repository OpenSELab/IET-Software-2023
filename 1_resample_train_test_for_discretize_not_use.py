# -*- coding: utf-8 -*-
"""1-resample_train_test_for_discretize_not_use

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J_n87fNAoIdSvO6DXSCOB_1LU0AZX5d3
"""

!pip install numpy
!pip install python-javabridge
!pip install python-weka-wrapper3
# weka can run in colab

import weka.core.jvm as jvm
jvm.start(system_info=True)

# Commented out IPython magic to ensure Python compatibility.
import scipy.io.arff
import pandas as pd
import numpy as np
from sklearn.utils import resample # for Bootstrap sampling
import shutil
import os
import ast


from numpy import array


#Import graphical plotting libraries
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

#Import resampling and modeling algorithms

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import KBinsDiscretizer

from sklearn.model_selection import train_test_split

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import matthews_corrcoef
import warnings

from weka.filters import Filter
from weka.core.converters import Loader



warnings.filterwarnings("ignore")

# click load google drive
rootpath = "/content/drive/MyDrive/Colab Notebooks/1/"

datasets_original = "datasets-original/"
datasets_discretize = "datasets-discretize/"


AEEEM = ["EQ"]
ReLink = ["Zxing", "Apache", "Safe"]
Promise = [ "camel-1.2", "ivy-1.1", "jedit-3.2", "log4j-1.1", "lucene-2.0", "lucene-2.2", "lucene-2.4",
        "poi-1.5", "poi-2.5", "poi-3.0", "xalan-2.5", "xalan-2.6"]

ARFF = "ARFF/"

CSV = "CSV/"
PythonCSV = "PythonCSV/"

PythonBOOTSTRAP = "PythonBOOTSTRAP/"

CLS = [RandomForestClassifier(),LogisticRegression(),GaussianNB(),DecisionTreeClassifier(),KNeighborsClassifier(),MLPClassifier()]

# some data sets imbanlance so we exclude it because these classifier have precision recall and f1 = 0  so we select 30-70% percent of defective

loader = Loader(classname="weka.core.converters.ArffLoader")

readfilepath = rootpath + datasets_original + ARFF

outfilepath = rootpath + datasets_discretize

readfile = readfilepath + "AEEEM/" + AEEEM[0] + ".arff"

data = loader.load_file(readfile)
print(data)

discretize = Filter(classname="weka.filters.unsupervised.attribute.Discretize")
discretize.inputformat(data)
discretized = discretize.filter(data)

print(discretized)

df = pd.DataFrame(discretized)
print(df)

# python for discretize  files save in PythonCSV
# read csv files
readfilepath = rootpath + datasets_original + CSV

outfilepath = rootpath + datasets_discretize + PythonCSV


for i in range(len(AEEEM)):

  readfile = readfilepath + "AEEEM/" + AEEEM[i] + ".csv"
  data = pd.read_csv(readfile)

  df = pd.DataFrame(data)
  #print(df['CvsEntropy'][0])
  #df['bug'][0].decode('utf-8')

  #print(list(df))
  attr = list(df)
  #print(len(attr))

  print(df)
  # 用KBinsDiscretizer转换数据集
  enc = KBinsDiscretizer(n_bins=10, encode='onehot').fit_transform(df)
  print(enc)


"""
# save as csv files
out_file = outfilepath + "AEEEM/" + AEEEM[i] + ".csv"
setDir(out_file)
out_file_performance = pd.DataFrame(performance_list)

out_file_performance.to_csv(out_file,index=False,columns=["project","cls_boot","accuracy","precision","recall","f1","auc","mcc"])
"""

def setDir(filepath):
    #if filepath not exist then create  ！

    if not os.path.exists(filepath):
        pass
    else:
        shutil.rmtree(filepath,ignore_errors=True)

# Lets configure Bootstrap
sample_times = 25  #No. of bootstrap samples to be repeated (created) seed is 0-24
# Lets run Bootstrap
# change the datasets name in turn

readfilepath = rootpath + datasets_discretize + ARFF

outfilepath = rootpath + datasets_discretize

performance_list = list()
for i in range(len(AEEEM)):

  readfile = readfilepath + "AEEEM/" + AEEEM[i] + ".arff"


  data,meta = scipy.io.arff.loadarff(readfile) # NOTE: ReLink original has bug{Y,N}->error  correct is bug {Y,N}


  df = pd.DataFrame(data)
  #print(df['CvsEntropy'][0])
  #df['bug'][0].decode('utf-8')

  #print(list(df))
  attr = list(df)
  #print(len(attr))

  print(df)
  for m in range(len(attr)):
    df[attr[m]] = df[attr[m]].str.decode('utf-8')  #df['bug'] = df['bug'].str.decode('utf-8') #remove b
    df[attr[m]] = df[attr[m]].str.replace('\\', '') #remove \
    df[attr[m]] = df[attr[m]].str.replace('\'', '') #remove '


  # bug has a 'Y' and 'N'
  df['bug'] = (df['bug']== "Y").astype(int)  # then bug into N->0 Y->1 !!!

  print(df)





  print(performance_list)

  for j in range(len(CLS)):

    #accuracy_list = list()
    #precision_list = list()
    #recall_list = list()
    #f1_list = list()
    #auc_list = list()
    #mcc_list = list()

    # Bootstrap
    for k in range(sample_times):

      #prepare train & test sets
      #Sampling with replacement..whichever is not used in training data will be used in test data
      train = resample(df, random_state = k)

      #picking rest of the data not considered in training sample test = df - train
      test = pd.concat([df, train, train]).drop_duplicates(keep = False)

      #print(train)
      #print(test)

      train = np.array(train)
      test = np.array(test)

      #fit model
      model = CLS[j] # can change max_iter=1000
      model.fit(train[:,:-1], train[:,-1]) #model.fit(X_train,y_train) i.e model.fit(train set, train label as it is a classifier)
      #evaluate model
      predictions = model.predict(test[:,:-1]) #model.predict(X_test)


      accuracy = accuracy_score(test[:,-1], predictions) #accuracy_score(y_test, y_pred)
      precision = precision_score(test[:,-1], predictions)
      recall = recall_score(test[:,-1], predictions)
      f1 = f1_score(test[:,-1], predictions)
      auc = roc_auc_score(test[:,-1], predictions)
      mcc = matthews_corrcoef(test[:,-1], predictions)

      performance_list.append((AEEEM[i],str(CLS[j])+str(k),accuracy,precision,recall,f1,auc,mcc))

      #caution, overall accuracy score can mislead when classes are imbalanced
      #accuracy_list.append(accuracy)
      #precision_list.append(precision)
      #recall_list.append(recall)
      #f1_list.append(f1)
      #auc_list.append(auc)
      #mcc_list.append(mcc)

print("end")
print(performance_list)
# save as csv files
out_file = outfilepath + "AEEEM-performance.csv"
setDir(out_file)
out_file_performance = pd.DataFrame(performance_list,columns=["project","cls_boot","accuracy","precision","recall","f1","auc","mcc"])

out_file_performance.to_csv(out_file,index=False,columns=["project","cls_boot","accuracy","precision","recall","f1","auc","mcc"])








# performance end

"""**下面是草稿**"""

#Lets configure Bootstrap
n_iterations = 25  #No. of bootstrap samples to be repeated (created)
#Lets run Bootstrap

for i in range(n_iterations):
    #prepare train & test sets
    #Sampling with replacement..whichever is not used in training data will be used in test data
    train = resample(df,random_state=i)

    #picking rest of the data not considered in training sample test = df - train
    test = pd.concat([df, train, train]).drop_duplicates(keep=False)








    # save as csv files
    out_file_train = outfilepath + AEEEM[i] + "-train-" + str(k) + ".csv"
    out_file_test = outfilepath + AEEEM[i] + "-test-" + str(k) + ".csv"

    output_train = pd.DataFrame(train)
    output_test = pd.DataFrame(test)
    output_train.to_csv(out_file_train,index=False)
    output_test.to_csv(out_file_test,index=False)

# bootstrap finished but ReLink original-datasets cannot resample because of the files in it bug{Y,N}->error miss a space, crrect is bug {Y,N}

# Lets configure Bootstrap
sample_times = 25  #No. of bootstrap samples to be repeated (created) seed is 0-24
# Lets run Bootstrap
# change the datasets name in turn

readfilepath = rootpath + datasets_original + CSV

outfilepath = rootpath + datasets_discretize

performance_list = list()
for i in range(len(AEEEM)):

  readfile = readfilepath + "AEEEM/" + AEEEM[i] + ".csv"
  data = pd.read_csv(readfile)

  data,meta = scipy.io.arff.loadarff(readfile) # NOTE: ReLink original has bug{Y,N}->error  correct is bug {Y,N}


  df = pd.DataFrame(data)
  #print(df['CvsEntropy'][0])
  #df['bug'][0].decode('utf-8')

  #print(list(df))
  attr = list(df)
  #print(len(attr))

  print(df)
  for m in range(len(attr)):
    df[attr[m]] = df[attr[m]].str.decode('utf-8')  #df['bug'] = df['bug'].str.decode('utf-8') #remove b
    df[attr[m]] = df[attr[m]].str.replace('\\', '') #remove \
    df[attr[m]] = df[attr[m]].str.replace('\'', '') #remove '


  # bug has a 'Y' and 'N'
  df['bug'] = (df['bug']== "Y").astype(int)  # then bug into N->0 Y->1 !!!

  print(df)





  print(performance_list)

  for j in range(len(CLS)):

    #accuracy_list = list()
    #precision_list = list()
    #recall_list = list()
    #f1_list = list()
    #auc_list = list()
    #mcc_list = list()

    # Bootstrap
    for k in range(sample_times):

      #prepare train & test sets
      #Sampling with replacement..whichever is not used in training data will be used in test data
      train = resample(df, random_state = k)

      #picking rest of the data not considered in training sample test = df - train
      test = pd.concat([df, train, train]).drop_duplicates(keep = False)

      #print(train)
      #print(test)

      train = np.array(train)
      test = np.array(test)

      #fit model
      model = CLS[j] # can change max_iter=1000
      model.fit(train[:,:-1], train[:,-1]) #model.fit(X_train,y_train) i.e model.fit(train set, train label as it is a classifier)
      #evaluate model
      predictions = model.predict(test[:,:-1]) #model.predict(X_test)


      accuracy = accuracy_score(test[:,-1], predictions) #accuracy_score(y_test, y_pred)
      precision = precision_score(test[:,-1], predictions)
      recall = recall_score(test[:,-1], predictions)
      f1 = f1_score(test[:,-1], predictions)
      auc = roc_auc_score(test[:,-1], predictions)
      mcc = matthews_corrcoef(test[:,-1], predictions)

      performance_list.append((AEEEM[i],str(CLS[j])+str(k),accuracy,precision,recall,f1,auc,mcc))

      #caution, overall accuracy score can mislead when classes are imbalanced
      #accuracy_list.append(accuracy)
      #precision_list.append(precision)
      #recall_list.append(recall)
      #f1_list.append(f1)
      #auc_list.append(auc)
      #mcc_list.append(mcc)

print("end")
print(performance_list)
# save as csv files
out_file = outfilepath + "AEEEM-performance.csv"
setDir(out_file)
out_file_performance = pd.DataFrame(performance_list,columns=["project","cls_boot","accuracy","precision","recall","f1","auc","mcc"])

out_file_performance.to_csv(out_file,index=False,columns=["project","cls_boot","accuracy","precision","recall","f1","auc","mcc"])








# performance end

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.utils import resample # for Bootstrap sampling
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import KBinsDiscretizer
from sklearn.tree import DecisionTreeRegressor

# click load google drive
rootpath = "/content/drive/MyDrive/Colab Notebooks/1/"

datasets_original = "datasets-original/"
datasets_discretize = "datasets-discretize/"


AEEEM = ["EQ"]
ReLink = ["Zxing", "Apache", "Safe"]
Promise = [ "camel-1.2", "ivy-1.1", "jedit-3.2", "log4j-1.1", "lucene-2.0", "lucene-2.2", "lucene-2.4",
        "poi-1.5", "poi-2.5", "poi-3.0", "xalan-2.5", "xalan-2.6"]

ARFF = "ARFF/"

CSV = "CSV/"
PythonCSV = "PythonCSV/"

readfilepath = rootpath + datasets_original + CSV

outfilepath = rootpath + datasets_original

performance_list = list()
readfile = readfilepath + "AEEEM/" + AEEEM[0] + ".csv"
data = pd.read_csv(readfile)
df = pd.DataFrame(data)

attr_bug = list(df)
attr = attr_bug[:-1]

train = resample(df, random_state = 0)
#picking rest of the data not considered in training sample test = df - train
test = pd.concat([df, train, train]).drop_duplicates(keep = False)

print(df[attr].head())

# 用KBinsDiscretizer转换数据集  标签列不需要离散化 但是特征离散化后如果进行特征重要性计算会发现一个特征被分割了 准备分割后的特征子集 计算重要性求和=原特征重要性 暂时不做，留待日后补实验
enc = KBinsDiscretizer(n_bins=10, encode='onehot', strategy='uniform')
X_binned = enc.fit_transform(df[attr])
print(X_binned.toarray())
print(X_binned.toarray().shape)


#X=data.iloc[:,0].values.reshape(-1,1)
#print(X)

#test=KBinsDiscretizer(n_bins=10,encode='onehot',strategy='uniform').fit_transform(X).toarray()
#print(test)
#model.fit(train[:,:-1], train[:,-1]) #model.fit(X_train,y_train) i.e model.fit(train set, train label as it is a classifier)
#evaluate model
#predictions = model.predict(test[:,:-1]) #model.predict(X_test)



# cut()函数用于进行等宽分箱，参数1是指待分箱的列，参数2是指分箱个数
#data_cut = pd.cut(df['ck_oo_numberOfPrivateMethods'],10)
#data_cut = pd.cut(df,10,precision=2,labels = attr)

# 获取每个分箱中的样本数
# data['ck_oo_numberOfPrivateMethods'].groupby(data_cut).count()

import numpy as np
import matplotlib.pyplot as plt

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import KBinsDiscretizer
from sklearn.tree import DecisionTreeRegressor

X = np.array([[ -3., 5., 15 ],
        [  0., 6., 14 ],
        [  6., 3., 11 ]])

X=X.reshape(-1,1)
print(X)
est = KBinsDiscretizer(n_bins=3,encode='onehot').fit(X)
print(est.fit_transform(X))
print(est.fit_transform(X).toarray())