# -*- coding: utf-8 -*-
"""1-resample_for_see_details

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18jxVeB6JYVZJRupEpb9mVMj9MEgkZk-e
"""

!pip install liac-arff # install arff but no use

import scipy.io.arff
import pandas as pd
import numpy as np
from sklearn.utils import resample # for Bootstrap sampling
import shutil
import os

# use to load ReLink original datasets, but dont need this
# import arff

# click load google drive
rootpath = "/content/drive/MyDrive/Colab Notebooks/1/"

datasets_original = "datasets-original/"
datasets_discretize = "datasets-discretize/"
datasets_log = "datasets-log/"
datasets_minmax = "datasets-min-max/"
datasets_standardize = "datasets-standardize/"

AEEEM = ["EQ", "JDT", "LC", "ML", "PDE"]
ReLink = ["Zxing", "Apache", "Safe"]
Columba = ["columba", "eclipse", "scarab"]
Promise = ["ant-1.3", "ant-1.4", "ant-1.5", "ant-1.6", "ant-1.7",
           "camel-1.0", "camel-1.2", "camel-1.4", "camel-1.6",
           "ivy-1.1", "ivy-1.4", "ivy-2.0",
           "jedit-3.2", "jedit-4.0", "jedit-4.1", "jedit-4.2", "jedit-4.3",
           "log4j-1.0", "log4j-1.1", "log4j-1.2",
           "lucene-2.0", "lucene-2.2", "lucene-2.4",
           "poi-1.5", "poi-2.0", "poi-2.5", "poi-3.0",
           "xalan-2.4", "xalan-2.5", "xalan-2.6", "xalan-2.7",
           "xerces-1.2", "xerces-1.3", "xerces-1.4"]

ARFF = "ARFF/"
CSV = "CSV/"

BOOTSTRAP = "BOOTSTRAP/"

def setDir(filepath):

    #if filepath not exist then create else delete all files then create ！

    if not os.path.exists(filepath):
        os.mkdir(filepath)
    else:
        shutil.rmtree(filepath,ignore_errors=True)
        os.mkdir(filepath)

# Lets configure Bootstrap
n_iterations = 25  #No. of bootstrap samples to be repeated (created) seed is 0-24
# Lets run Bootstrap
# change the datasets name in turn

readfilepath = rootpath + datasets_standardize + ARFF

for i in range(len(Promise)):

  readfile = readfilepath + "Promise/" + Promise[i] + ".arff"
  data,meta = scipy.io.arff.loadarff(readfile) # NOTE: ReLink original has bug{Y,N}->error  correct is bug {Y,N}

  df = pd.DataFrame(data)

  # bug has a b'Y' and b'N'
  df["bug"] = (df["bug"]== b"Y").astype(int)  # then bug into N->0 Y->1 !!! all the bug label change to 1

  outfilepath = rootpath + datasets_standardize + BOOTSTRAP + Promise[i] + "/"
  setDir(outfilepath)

  # Bootstrap
  for k in range(n_iterations):
    #prepare train & test sets
    #Sampling with replacement..whichever is not used in training data will be used in test data
    train = resample(df, random_state = k)

    #picking rest of the data not considered in training sample test = df - train
    test = pd.concat([df, train, train]).drop_duplicates(keep = False)

    # save as csv files
    out_file_train = outfilepath + Promise[i] + "-train-" + str(k) + ".csv"
    out_file_test = outfilepath + Promise[i] + "-test-" + str(k) + ".csv"

    output_train = pd.DataFrame(train)
    output_test = pd.DataFrame(test)
    output_train.to_csv(out_file_train,index=False)
    output_test.to_csv(out_file_test,index=False)

# bootstrap finished but ReLink original-datasets cannot resample because of the files in it bug{Y,N}->error miss a space, crrect is bug {Y,N}



"""**下面是草稿**"""

#rawData = scipy.io.arff.loadarff(rootpath+datasets_original+"AEEEM/EQ.arff")
data,meta = scipy.io.arff.loadarff(rootpath+datasets_original+ARFF+"AEEEM/EQ.arff")

# rawData[0] is data rawData[1] is features
# df = pd.DataFrame(rawData[0])

df = pd.DataFrame(data)
print(df.shape)

#Lets configure Bootstrap
n_iterations = 25  #No. of bootstrap samples to be repeated (created)
#Lets run Bootstrap

for i in range(n_iterations):
    #prepare train & test sets
    #Sampling with replacement..whichever is not used in training data will be used in test data
    train = resample(df,random_state=i)

    #picking rest of the data not considered in training sample test = df - train
    test = pd.concat([df, train, train]).drop_duplicates(keep=False)

    print(df.shape)
    print(train.shape)
    print(test.shape)

# arff2csv now without do it

file_name = rootpath+datasets_original+ARFF+"AEEEM/EQ.arff"
data,meta = scipy.io.arff.loadarff(file_name)

#print(data)
#print(meta)

df=pd.DataFrame(data)
#print(df.head())
#print(df)

# save as csv files
out_file = rootpath+datasets_original+CSV
output=pd.DataFrame(df)
output.to_csv(out_file+"AEEEM/EQ.CSV",index=False)

import copy

import imblearn
import scipy.io.arff
import pandas
import arff
import sys
import os
import random
from imblearn.over_sampling import SMOTE, RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from pathlib import Path
from sklearn.model_selection import train_test_split
import numpy.random
from varclushi import VarClusHi


def resample_smote(cwd, x, y, datasetname="unknown"):
    resampler = SMOTE()
    try:
        x_resampled, y_resampled = resampler.fit_resample(x, y)
    except ValueError:
        x_resampled, y_resampled = RandomOverSampler().fit_resample(x, y)
    write_arff(cwd + "3.smote.arff", x_resampled, y_resampled, datasetname+"-smote")


def resample_oversampling(cwd, x, y, datasetname="unknown"):
    resampler = RandomOverSampler()
    x_resampled, y_resampled = resampler.fit_resample(x, y)
    write_arff(cwd + "2.oversampling.arff", x_resampled, y_resampled, datasetname+"-oversampling")


def resample_undersampling(cwd, x, y, datasetname="unknown"):
    resampler = RandomUnderSampler()
    x_resampled, y_resampled = resampler.fit_resample(x, y)
    write_arff(cwd + "4.undersampling.arff", x_resampled, y_resampled, datasetname+"undersampling")


def write_arff(filename, x, y, datasetname="unknown"):
    fd = open(filename, "w+")
    fd.write("@relation data/" + datasetname + ".csv-weka.filters.unsupervised.attribute.Remove-R1\n\n")
    for column in x.columns:
        fd.write("@attribute " + str(column) + " numeric\n")
    fd.write("@attribute label {0,1}\n\n@data\n")

    for i in range(0, y.shape[0]):
        values = []
        for elem in x.iloc[i, :]:
            values.append(str(elem))
        values.append(str(int(y.iloc[i])))
        fd.write(",".join(values) + "\n")
    fd.close()


def spearman(df, threshold):
    corr = df.corr(method="spearman")
    count = 0
    for i in range(0, len(df.columns)):
        for j in range(i + 1, len(df.columns)):
            if abs(corr.iloc[i, j]) > threshold:
                count += 1
    return count


def eliminate_corr(xx):
    clusters = VarClusHi(xx).varclus().rsquare
    sorted_attrs = []
    for cluster_id in clusters.groupby("Cluster").sum().index:
        sorted_attrs.append(clusters[(clusters["Cluster"] == cluster_id)].sort_values(by=["RS_Ratio"]).iloc[:, [1, 4]])

    cluster_attrs = []

    for cluster_id in clusters.groupby("Cluster").sum().index:
        group_attrs = []
        for i in range(0, len(sorted_attrs[cluster_id])):
            group_attrs.append(sorted_attrs[cluster_id].iloc[i, 0])
        while True:
            spc = spearman(xx.loc[:, group_attrs], 0.7)
            if spc > 0:
                group_attrs.pop(-1)
            else:
                break
        cluster_attrs.extend(group_attrs)
    return xx.loc[:,cluster_attrs]

def main(corr=True):
    dataset = ""
    project = ""
    work_dir = ""

    for arg in sys.argv:
        if arg.startswith("--dataset"):
            dataset = arg.split("=")[1]
        if arg.startswith("--project"):
            project = arg.split("=")[1]
        if arg.startswith("--cwd"):
            work_dir = arg.split("=")[1]

    if dataset == "" or project == "":
        raise ValueError("dataset and project must be specified.")

    if work_dir == "":
        raise ValueError("cwd must be specified.")

    rawData = scipy.io.arff.loadarff("./dataset/" + dataset + "/" + project + ".arff")

    df = pandas.DataFrame(rawData[0])
    if "Defective" in df.columns or "DEFECTIVE" in df.columns:
        # NASA MDP Dataset
        if "Defective" in df.columns:
            df["DEFECTIVE"] = (df["Defective"] == b"Y").astype(int)
            df = df.drop(columns=["Defective"])
            pass
        else:
            df["DEFECTIVE"] = (df["label"] == b"Y").astype(int)
            df = df.drop(columns=["label"])
            pass
        columns = df.columns.values.tolist()
        columns.remove("DEFECTIVE")
        x = df[columns].copy()
        y = df[["DEFECTIVE"]].copy()
    elif "class" in df.columns:
        # AEEEM Dataset
        df["DEFECTIVE"] = (df["class"] == b"buggy").astype(int)
        df = df.drop(columns=["class"])
        columns = df.columns.values.tolist()
        columns.remove("DEFECTIVE")
        x = df[columns].copy()
        y = df[["DEFECTIVE"]].copy()
    elif "label" in df.columns:
        # NASA JM1
        df["DEFECTIVE"] = (df["label"] == b"Y").astype(int)
        df = df.drop(columns=["label"])
        columns = df.columns.values.tolist()
        columns.remove("DEFECTIVE")
        x = df[columns].copy()
        y = df[["DEFECTIVE"]].copy()
    elif "isDefective" in df.columns:
        # Relink Dataset
        df["DEFECTIVE"] = (df["isDefective"] == b"buggy").astype(int)
        df = df.drop(columns=["isDefective"])
        columns = df.columns.values.tolist()
        columns.remove("DEFECTIVE")
        x = df[columns].copy()
        y = df[["DEFECTIVE"]].copy()
    elif "_class_" in df.columns:
        # Columba Dataset
        df["DEFECTIVE"] = (df["_class_"] == b"bug").astype(int)
        df = df.drop(columns=["_class_"])
        columns = df.columns.values.tolist()
        columns.remove("DEFECTIVE")
        x = df[columns].copy()
        y = df[["DEFECTIVE"]].copy()
    else:
        # PROMISE Dataset
        df["DEFECTIVE"] = (df["bug"] != b"0").astype(int)
        df = df.drop(columns=["bug"])
        columns = df.columns.values.tolist()
        columns.remove("DEFECTIVE")
        x = df[columns].copy()
        y = df[["DEFECTIVE"]].copy()

    dratio = y[y["DEFECTIVE"] == 1].count() / y.shape[0]
    print("Defective Ratio", y[y["DEFECTIVE"] == 1].count(), y.shape, dratio)

    x = eliminate_corr(x)
    x_columns = x.columns
    data = copy.deepcopy(x)
    data["defective"] = y

    random_seeds_str = []
    random_seeds = random.sample(range(0, 101), 25)
    for seed in random_seeds:
        random_seeds_str.append(str(seed))

    cwd = work_dir + "/" + dataset + "/" + project + "/"

    path_cwd = Path(cwd)
    if path_cwd.exists():
        import shutil
        shutil.rmtree(cwd)
    os.makedirs(cwd)

    # write sampling random seed data
    fd = open(cwd + "random_seeds.txt", "w+")
    fd.write(", ".join(random_seeds_str))
    fd.close()

    index = 0
    for seed in random_seeds:
        cwd_current_random_seed = cwd + str(index) + "/"
        os.makedirs(cwd_current_random_seed)

        # write sampling random seed data
        fd = open(cwd_current_random_seed + "seeds.txt", "w+")
        fd.write(str(seed))
        fd.close()

        train = data.sample(frac=1.0, replace=True, random_state=seed)
        test = data.loc[data.index.difference(train.index)].copy()

        train_x = train.loc[:, x_columns]
        train_y = train.loc[:, "defective"]
        test_x = test.loc[:, x_columns]
        test_y = test.loc[:, "defective"]

        write_arff(cwd_current_random_seed + "1.train_original.arff", train_x, train_y, project+"-original")
        write_arff(cwd_current_random_seed + "5.test.arff", test_x, test_y, project+"test")

        resample_smote(cwd_current_random_seed, train_x, train_y, project)
        resample_oversampling(cwd_current_random_seed, train_x, train_y, project)
        resample_undersampling(cwd_current_random_seed, train_x, train_y, project)
        index += 1

main()
"""
if __name__ == "__main__":

    works_aeeem = ["eq", "jdt", "lc2", "ml", "pde"]
    works_relink = ["zxing", "apache", "safe"]
    works_columba = ["columba", "org.eclipse.jdt.core", "scarab"]
    works_promise = ["ant-1.3", "ant-1.4", "ant-1.5", "ant-1.6", "ant-1.7",
                     "camel-1.0", "camel-1.2", "camel-1.4", "camel-1.6",
                     "ivy-1.1", "ivy-1.4", "ivy-2.0",
                     "jedit-3.2", "jedit-4.0", "jedit-4.1", "jedit-4.2", "jedit-4.3",
                     "log4j-1.0", "log4j-1.1",
                     "lucene-2.0", "lucene-2.2", "lucene-2.4",
                     "poi-1.5", "poi-2.0", "poi-2.5",
                     "xalan-2.4", "xalan-2.5", "xalan-2.6",
                     "xerces-1.2", "xerces-1.3", "xerces-1.4"]

    work_datasets = {"promise": works_promise, "aeeem": works_aeeem, "relink": works_relink, "columba":works_columba}
    # work_datasets = {"relink": works_relink}
    datasets_by_dr = [[],[],[],[],[]]
    datasets_str = ["0-10", "10-20", "20-30", "30-40", "40-50"]

    def dr2id(dr):
        if dr<0.1:
            return 0
        if dr<0.2:
            return 1
        if dr<0.3:
            return 2
        if dr<0.4:
            return 3
        if dr<=0.5:
            return 4
        return -1


    for dataset, projects in work_datasets.items():
        for project in projects:
            rawData = scipy.io.arff.loadarff("./dataset/" + dataset + "/" + project + ".arff")
            df = pandas.DataFrame(rawData[0])
            if "Defective" in df.columns or "DEFECTIVE" in df.columns:
                # NASA MDP Dataset
                if "Defective" in df.columns:
                    df["DEFECTIVE"] = (df["Defective"] == b"Y").astype(int)
                    df = df.drop(columns=["Defective"])
                    pass
                else:
                    df["DEFECTIVE"] = (df["label"] == b"Y").astype(int)
                    df = df.drop(columns=["label"])
                    pass
                columns = df.columns.values.tolist()
                columns.remove("DEFECTIVE")
                x = df[columns].copy()
                y = df[["DEFECTIVE"]].copy()
            elif "class" in df.columns:
                # AEEEM Dataset
                df["DEFECTIVE"] = (df["class"] == b"buggy").astype(int)
                df = df.drop(columns=["class"])
                columns = df.columns.values.tolist()
                columns.remove("DEFECTIVE")
                x = df[columns].copy()
                y = df[["DEFECTIVE"]].copy()
            elif "label" in df.columns:
                # NASA JM1
                df["DEFECTIVE"] = (df["label"] == b"Y").astype(int)
                df = df.drop(columns=["label"])
                columns = df.columns.values.tolist()
                columns.remove("DEFECTIVE")
                x = df[columns].copy()
                y = df[["DEFECTIVE"]].copy()
            elif "isDefective" in df.columns:
                # Relink Dataset
                df["DEFECTIVE"] = (df["isDefective"] == b"buggy").astype(int)
                df = df.drop(columns=["isDefective"])
                columns = df.columns.values.tolist()
                columns.remove("DEFECTIVE")
                x = df[columns].copy()
                y = df[["DEFECTIVE"]].copy()
            elif "_class_" in df.columns:
                # Columba Dataset
                df["DEFECTIVE"] = (df["_class_"] == b"bug").astype(int)
                df = df.drop(columns=["_class_"])
                columns = df.columns.values.tolist()
                columns.remove("DEFECTIVE")
                x = df[columns].copy()
                y = df[["DEFECTIVE"]].copy()
            else:
                # PROMISE Dataset
                df["DEFECTIVE"] = (df["bug"] != b"0").astype(int)
                df = df.drop(columns=["bug"])
                columns = df.columns.values.tolist()
                columns.remove("DEFECTIVE")
                x = df[columns].copy()
                y = df[["DEFECTIVE"]].copy()

            x = eliminate_corr(x)

            count_col = y[y["DEFECTIVE"] == 1].count()

            dratio = count_col.iloc[0] / y.shape[0]

            print(dataset, project, y.shape[0], dratio)

            drid = dr2id(dratio)


            if drid >= 0:
                datasets_by_dr[drid].append((dataset, project, x, y))

    i=0
    for datasets_dr in datasets_by_dr:
        for datasetname, projectname, x, y in datasets_dr:
            cwd = "G:/experiment/original-fulltrain-non-corr/" + datasets_str[i] + "/"
            path_cwd = Path(cwd)
            if not path_cwd.exists():
                os.makedirs(cwd)
            write_arff("G:/experiment/original-fulltrain-non-corr/" + datasets_str[i] + "/" +datasetname+"."+projectname+".arff", x, y, datasetname+"."+projectname)
        i += 1
"""

"    ant_1_3_data_source = \"testdata/ant-1.3.csv\" \n",
    "    ant_1_4_data_source = \"testdata/ant-1.4.csv\" \n",
    "    ant_1_5_data_source = \"testdata/ant-1.5.csv\"\n",
    "    ant_1_6_data_source = \"testdata/ant-1.6.csv\"\n",
    "    ant_1_7_data_source = \"testdata/ant-1.7.csv\"\n",
    "    camel_1_0_data_source = \"testdata/camel-1.0.csv\"\n",
    "    camel_1_2_data_source = \"testdata/camel-1.2.csv\"\n",
    "    camel_1_4_data_source = \"testdata/camel-1.4.csv\"\n",
    "    camel_1_6_data_source = \"testdata/camel-1.6.csv\"\n",
    "    ivy_1_1_data_source = \"testdata/ivy-1.1.csv\"\n",
    "    ivy_1_4_data_source = \"testdata/ivy-1.4.csv\"\n",
    "    ivy_2_0_data_source = \"testdata/ivy-2.0.csv\"\n",
    "    jedit_3_2_data_source = \"testdata/jedit-3.2.csv\"\n",
    "    jedit_4_0_data_source = \"testdata/jedit-4.0.csv\"\n",
    "    jedit_4_1_data_source = \"testdata/jedit-4.1.csv\"\n",
    "    jedit_4_2_data_source = \"testdata/jedit-4.2.csv\"\n",
    "    jedit_4_3_data_source = \"testdata/jedit-4.3.csv\"\n",
    "    log4j_1_0_data_source = \"testdata/log4j-1.0.csv\"\n",
    "    log4j_1_1_data_source = \"testdata/log4j-1.1.csv\"\n",
    "    log4j_1_2_data_source = \"testdata/log4j-1.2.csv\"\n",
    "    lucene_2_0_data_source = \"testdata/lucene-2.0.csv\"\n",
    "    lucene_2_2_data_source = \"testdata/lucene-2.2.csv\"\n",
    "    lucene_2_4_data_source = \"testdata/lucene-2.4.csv\"\n",
    "    poi_1_5_data_source = \"testdata/poi-1.5.csv\"\n",
    "    poi_2_0_data_source = \"testdata/poi-2.0.csv\"\n",
    "    poi_2_5_data_source = \"testdata/poi-2.5.csv\"\n",
    "    poi_3_0_data_source = \"testdata/poi-3.0.csv\"\n",
    "    synapse_1_0_data_source = \"testdata/synapse-1.0.csv\"\n",
    "    synapse_1_1_data_source = \"testdata/synapse-1.1.csv\"\n",
    "    synapse_1_2_data_source = \"testdata/synapse-1.2.csv\"\n",
    "    tomcat_data_source = \"testdata/tomcat.csv\"\n",
    "    velocity_1_4_data_source = \"testdata/velocity-1.4.csv\"\n",
    "    velocity_1_5_data_source = \"testdata/velocity-1.5.csv\"\n",
    "    velocity_1_6_data_source = \"testdata/velocity-1.6.csv\"\n",
    "    xalan_2_4_data_source = \"testdata/xalan-2.4.csv\"\n",
    "    xalan_2_5_data_source = \"testdata/xalan-2.5.csv\"\n",
    "    xalan_2_6_data_source = \"testdata/xalan-2.6.csv\"\n",
    "    xalan_2_7_data_source = \"testdata/xalan-2.7.csv\"\n",
    "    xerces_1_2_data_source = \"testdata/xerces-1.2.csv\"\n",
    "    xerces_1_3_data_source = \"testdata/xerces-1.3.csv\"\n",
    "    xerces_1_4_data_source = \"testdata/xerces-1.4.csv\"\n",
    "\n",
    "    versionSource = (ant_1_3_data_source,ant_1_4_data_source,ant_1_5_data_source,ant_1_6_data_source,ant_1_7_data_source,\n",
    "    camel_1_0_data_source,camel_1_2_data_source,camel_1_4_data_source,camel_1_6_data_source,\n",
    "    ivy_1_1_data_source,ivy_1_4_data_source,ivy_2_0_data_source,jedit_3_2_data_source,jedit_4_0_data_source,\n",
    "    jedit_4_1_data_source,jedit_4_2_data_source,jedit_4_3_data_source,log4j_1_0_data_source,log4j_1_1_data_source,\n",
    "    log4j_1_2_data_source,lucene_2_0_data_source,lucene_2_2_data_source,lucene_2_4_data_source,poi_1_5_data_source,\n",
    "    poi_2_0_data_source,poi_2_5_data_source,poi_3_0_data_source,synapse_1_0_data_source,synapse_1_1_data_source,\n",
    "    synapse_1_2_data_source,tomcat_data_source,velocity_1_4_data_source,velocity_1_5_data_source,\n",
    "    velocity_1_6_data_source,xalan_2_4_data_source,xalan_2_5_data_source,xalan_2_6_data_source,xalan_2_7_data_source,\n",
    "    xerces_1_2_data_source,xerces_1_3_data_source,xerces_1_4_data_source)\n",
    "Apache = \"ReLinkCSV/Apache.csv\" \n",
    "Zxing = \"ReLinkCSV/Zxing.csv\"\n",
    "Safe = \"ReLinkCSV/Safe.csv\"\n",
    "\n",
    "#\n",
    "EQ = \"AEEEMCSV/EQ.csv\"\n",
    "JDT = \"AEEEMCSV/JDT.csv\"\n",
    "LC = \"AEEEMCSV/LC.csv\"\n",
    "ML = \"AEEEMCSV/ML.csv\"\n",
    "PDE = \"AEEEMCSV/PDE.csv\"\n",